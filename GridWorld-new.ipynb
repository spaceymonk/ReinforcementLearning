{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6266208f",
   "metadata": {},
   "source": [
    "# GridWorld Solution\n",
    "\n",
    "Written by Berktug Ozkan.\n",
    "\n",
    "Related post link, [click here](https://spaceymonk.github.io/gridworld-rl-again.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecbb09",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c66494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79b2af",
   "metadata": {},
   "source": [
    "## Creating Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c99a9",
   "metadata": {},
   "source": [
    "### Action & Observation Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7eaa1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space      = (4,)  # right, left, up, down\n",
    "observation_space = (3, 4)  # the default dimensions of Frozen Lake problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfc11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size      = np.cumprod(action_space, dtype=np.int32)[-1]\n",
    "observation_space_size = np.cumprod(observation_space, dtype=np.int32)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5a4c8",
   "metadata": {},
   "source": [
    "### Q Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cd8fc",
   "metadata": {},
   "source": [
    "Stores the quality values of state--action pairs.\n",
    "\n",
    "  * _rows_, states (0 to 11)\n",
    "  * _columns_, actions (0 to 3; right, left, up, down respectively)\n",
    "\n",
    "I have tried to preserve the shape of the [Empty Q Table](https://spaceymonk.github.io/assets/2022-08-02/GridWorld_-_Q_Table.pdf) mentioned in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c41d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create current Q values table to fill\n",
    "current_q_table = np.zeros( (observation_space_size, action_space_size) )\n",
    "\n",
    "# we know the values of the terminal states\n",
    "current_q_table[3] = np.ones(action_space_size)\n",
    "current_q_table[7] = -np.ones(action_space_size)\n",
    "\n",
    "current_q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef77a20",
   "metadata": {},
   "source": [
    "### State Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3ab8b",
   "metadata": {},
   "source": [
    "Value function of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d45631a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., -1.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value table to fill, it will act as observations\n",
    "states = np.zeros(observation_space)\n",
    "\n",
    "# we know the values of the terminal states\n",
    "states[np.unravel_index(3, observation_space)] = 1.00\n",
    "states[np.unravel_index(7, observation_space)] = -1.00\n",
    "\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92f85c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823eebf",
   "metadata": {},
   "source": [
    "Main loop to fit Q table. You can change the [parameters](#Parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44b4d2",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f52e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 100  # number iterations to run the loop\n",
    "gamma        = 1  # discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f1deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = -0.04  # for every move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da76f9a",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdbf3d",
   "metadata": {},
   "source": [
    "#### Handling \"bumbing\" on the walls\n",
    "\n",
    "Below cell defines a function to check whether the given state is a wall or not.\n",
    "\n",
    "By defining such function it is easy to adapt other mazes. For example, if \"state 1\" becomes a wall in another\n",
    "maze, we can simply add:\n",
    "\n",
    "``` python\n",
    "return row * observation_space[1] + col == 5 or row * observation_space[1] + col == 1\n",
    "```\n",
    "\n",
    "One may want to introduce an array of \"wall\" states to shorten the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30265239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_wall(row, col):\n",
    "    return row * observation_space[1] + col == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1e9c5",
   "metadata": {},
   "source": [
    "#### A different approach!\n",
    "\n",
    "If you haven't noticed yet, I hard-coded the terminal and wall states. Keep this in mind if you want to introduce another maze.\n",
    "\n",
    "(1) If you look at the line 30 where we update the state values, we are doing this inside the loop so every state uses the *brand new estimated value* of the neighbouring state.\n",
    "\n",
    "(2) On the other hand, on the commented line 37, you can see the update operation can be done at the end of the loop. If you choose this approach, estimated values uses the previous loop's values. _Do not forget to remove the first `states.copy()` in the history array (line 2)._\n",
    "\n",
    "So which one? In my opinion;\n",
    "\n",
    " - I think this last option is more appropriate to parallelization due to the lack of data race. Data is only read and written on another memory block. But in the first case, data needs to be written and only then, it can be read.\n",
    " - But iteration count may change. Meaning the former may converge faster in terms of iterations. For example, it took,\n",
    "   | update per state (first case) | update all at once (second case) |\n",
    "   | --- | --- |\n",
    "   | 13 steps | 25 steps |\n",
    " - So, it really depends on the problem and available hardware. If there are so many states which perfectly fits into the GPU, then use the latter. Otherwise, you can use the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce18298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Values:\n",
      "[[ 0.81155822  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155822  0.          0.66027397 -1.        ]\n",
      " [ 0.70530822  0.65530822  0.61141553  0.38792491]]\n",
      "\n",
      "Q Table:\n",
      "[[ 0.81155822  0.76655822  0.77718322  0.73718322]\n",
      " [ 0.86780822  0.78280822  0.82718322  0.82718322]\n",
      " [ 0.91780822  0.81205479  0.8810274   0.675     ]\n",
      " [ 1.          1.          1.          1.        ]\n",
      " [ 0.72093322  0.72093322  0.76155822  0.67655822]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.68707763  0.64114155  0.66027397  0.41515982]\n",
      " [-1.         -1.         -1.         -1.        ]\n",
      " [ 0.63093322  0.67093322  0.70530822  0.66030822]\n",
      " [ 0.58019406  0.65530822  0.61591895  0.61591895]\n",
      " [ 0.39750888  0.61141553  0.59254249  0.55345573]\n",
      " [ 0.20913242  0.38792491 -0.74006596  0.37027397]]\n"
     ]
    }
   ],
   "source": [
    "q_table_history = []  # history of Q tables\n",
    "states_history  = [states.copy()]  # history of state values\n",
    "\n",
    "\n",
    "for i in range(0, n_iterations):\n",
    "    for state in range(observation_space_size):  # for every state,\n",
    "        \n",
    "        # if terminal state or wall encountered, ignore\n",
    "        if state == 3 or state == 5 or state == 7:\n",
    "            continue\n",
    "        \n",
    "        row, col = np.unravel_index(state, observation_space)\n",
    "\n",
    "        # neighbouring state values\n",
    "        right_neigbour = states[row, col+1] if col+1 >= 0 and col+1 < observation_space[1] and not is_wall(row, col+1) else states[row, col]\n",
    "        left_neigbour  = states[row, col-1] if col-1 >= 0 and col-1 < observation_space[1] and not is_wall(row, col-1) else states[row, col]\n",
    "        up_neigbour    = states[row-1, col] if row-1 >= 0 and row-1 < observation_space[0] and not is_wall(row-1, col) else states[row, col]\n",
    "        down_neigbour  = states[row+1, col] if row+1 >= 0 and row+1 < observation_space[0] and not is_wall(row+1, col) else states[row, col]\n",
    "        \n",
    "        # calculate action values\n",
    "        right = reward + gamma * (0.8 * right_neigbour + 0.10 * up_neigbour   + 0.10 * down_neigbour )\n",
    "        left  = reward + gamma * (0.8 * left_neigbour  + 0.10 * up_neigbour   + 0.10 * down_neigbour )\n",
    "        up    = reward + gamma * (0.8 * up_neigbour    + 0.10 * left_neigbour + 0.10 * right_neigbour)\n",
    "        down  = reward + gamma * (0.8 * down_neigbour  + 0.10 * left_neigbour + 0.10 * right_neigbour)\n",
    "        \n",
    "        # set action values to Q table in the current state\n",
    "        current_q_table[state] = np.array([right, left, up, down])\n",
    "        \n",
    "        # update state values\n",
    "        states[row, col] = np.max(current_q_table[state])\n",
    "    \n",
    "    # save to history\n",
    "    q_table_history.append(current_q_table.copy())\n",
    "    states_history.append(states.copy())\n",
    "    \n",
    "    # update states values\n",
    "#     states = np.max(current_q_table, axis=1).reshape(observation_space)\n",
    "\n",
    "print('State Values:')\n",
    "print(states)\n",
    "print('\\nQ Table:')\n",
    "print(current_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc499b",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f1c24",
   "metadata": {},
   "source": [
    "After training completed, the results are stored in `current_q_table` and `states`.\n",
    "\n",
    "To see the calculated values step by step you can use `q_table_history` and `states_history` arrays.\n",
    "\n",
    "---\n",
    "\n",
    "Below cell uses `ipywidgets` package to introduce an interactive slide bar to travel through time to see the calculation steps.\n",
    "\n",
    "If the output of the below cell did not rendered, please run the notebook yourselves.\n",
    "\n",
    "_Have fun!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "784e3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb7b0fbaebc41f0987212eb5428059e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='time_step', max=99), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact  # to use slidebar\n",
    "from IPython.display import display, HTML  # to use better formatting in Jupyter Notebook\n",
    "\n",
    "\n",
    "# Better display the 2D matrices\n",
    "def display_table(data):\n",
    "    return '<table><tr>{}</tr></table>'.format(\n",
    "        '</tr><tr>'.join('<td>{}</td>'.format(\n",
    "            '</td><td>'.join('{:0.3f}'.format(_) for _ in row)) for row in data))\n",
    "\n",
    "\n",
    "# Function will receive the slide bar's value with parameter time_step\n",
    "@interact(time_step=(0, n_iterations-1))\n",
    "def show_history(time_step=0):\n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"display:flex;flex-direction:row;align-items:center;justify-content:space-around;text-align:center;\">\n",
    "      <div><h4>State Values (before iteration) </h4> {display_table(states_history[time_step])}</div>\n",
    "      <div><h4>Q Values</h4> {display_table(q_table_history[time_step])}</div>\n",
    "      <div><h4>State Values (after iteration) </h4> {display_table(states_history[time_step+1]) if time_step+1 < n_iterations else None}</div>\n",
    "    </div>\n",
    "    \"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
